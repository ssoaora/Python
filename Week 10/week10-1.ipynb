{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from collections import deque\n",
    "```\n",
    "\n",
    "```python\n",
    "from collections import Counter\n",
    "```\n",
    "\n",
    "```python\n",
    "from collections import defaultdict\n",
    "```\n",
    "\n",
    "```python\n",
    "from collections import namedtuple\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Queue: deque(['Alice', 'Bob', 'Charlie', 'David', 'Eve'])\n",
      "Final Queue: deque(['Bob', 'David', 'Eve', 'Frank', 'Grace', 'Alice'])\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "\n",
    "# Step 1: Initialize deque with customers\n",
    "# A deque (double-ended queue) is initialized with customer names.\n",
    "queue = deque([\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\"])\n",
    "\n",
    "# Display the initial queue\n",
    "print(f\"Initial Queue: {queue}\")\n",
    "\n",
    "# Step 2: Adding new customers to the queue\n",
    "# 'append' adds a customer to the end, and 'appendleft' adds to the front.\n",
    "queue.append(\"Frank\")       # Adding Frank to the end\n",
    "queue.appendleft(\"Grace\")   # Adding Grace to the front\n",
    "\n",
    "# Step 3: Simulating ticket cancellation\n",
    "# 'remove' is used to cancel a specific customer's ticket.\n",
    "queue.remove('Charlie') # Remove Charlie from the queue\n",
    "\n",
    "# Step 4: Rotating the deque\n",
    "# Rotate moves elements cyclically; -2 means move two elements from the front to the back.\n",
    "queue.rotate(-2)\n",
    "\n",
    "# Display the final state of the queue\n",
    "print(f'Final Queue: {queue}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- deque 와 일반 리스트의 차이점\n",
    "    - 왜 deque가 리스트보다 효율적인지 고민\n",
    "    - 리스트로 구현했을 때의 시간 복잡도와 비교\n",
    "\n",
    "- 데이터 구조의 선택 기준\n",
    "    - 특정 상황에서 deque를 사용하는 것이 적합한 이유를 분석\n",
    "    - 고객 대기열, 작업 큐 관리 등의 실제 사례에 어떻게 적용될 수 있을지 고민\n",
    "\n",
    "- rotate 메서드의 활용\n",
    "    - 회전 연산이 실제로 어떤 시나리오에서 유용한지 고민\n",
    "    - 우선순위 변경, 원형 데이터 처리 등에 대한 활용 방안"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('text', 1), ('mining', 1), ('interdisciplinary', 1), ('field', 1), ('uses', 1)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "# Step 1: Define sample text\n",
    "# The text contains punctuation and a mix of uppercase and lowercase letters.\n",
    "text = '''Text mining is and interdisciplinary field that uses algorithms to derive patterns, information, and insights from texts.'''\n",
    "\n",
    "# Step 2: Clean the text\n",
    "# Remove punctuation and convert all letters to lowercase for uniformity.\n",
    "text_cleaned = text.translate(str.maketrans('', '', string.punctuation)).lower().split()\n",
    "\n",
    "# Step 3: Define stop words\n",
    "# Stop words are common words that are usually excluded from analysis.\n",
    "stop_words = {'is', 'an', 'to', 'and', 'from', 'that'}\n",
    "\n",
    "# Step 4: Count words excluding stop words\n",
    "# Use Counter to count the occurrences of each word, excluding stop words.\n",
    "word_counts = Counter(word for word in text_cleaned if word not in stop_words)\n",
    "\n",
    "# Step 5: Display the top 5 most frequent words\n",
    "# Print the 5 most common words with their frequencies.\n",
    "print(word_counts.most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 텍스트 전처리의 중요성\n",
    "    - 불필요한 기호 제거와 소문자 변환이 결과에 어떤 영향을 미치는지 고민.\n",
    "    - 특정 도메인(예: 법률, 의학)에서 적합한 전처리 방법 탐구.\n",
    "\n",
    "- Stop Words의 정의\n",
    "    - 어떤 단어를 불용어로 정의해야 하는지 고민.\n",
    "    - 분석의 목적에 따라 불용어 리스트를 어떻게 조정할 수 있을지.\n",
    "\n",
    "- 빈도 분석의 한계\n",
    "    - 단순히 빈도를 계산하는 것이 텍스트 의미를 충분히 반영할 수 있는지 고민.\n",
    "    - 반도 기반 접근법과 다른 분석 기법(예: TF-IDF, Word Embedding)의 비교."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Electronics: Smartphone, Laptop, Headphones\n",
      "Groceries: Apple, Milk\n",
      "Clothing: Shirt\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Step 1: List of products with categories\n",
    "# Each tuple contains a category and a product.\n",
    "products = [\n",
    "    ('Electronics', 'Smartphone'),\n",
    "    ('Electronics', 'Laptop'),\n",
    "    ('Groceries', 'Apple'),\n",
    "    ('Groceries', 'Milk'),\n",
    "    ('Clothing', 'Shirt'),\n",
    "    ('Electronics', 'Headphones')\n",
    "]\n",
    "\n",
    "# Step 2: Create a defaultdict to group products by category\n",
    "# defaultdict allows creating a list automatically for new keys.\n",
    "category_dict = defaultdict(list)\n",
    "\n",
    "# Step 3: Populate the defaultdict\n",
    "# Iterate through the products list and group items by category.\n",
    "for category, product in products:\n",
    "    category_dict[category].append(product)\n",
    "\n",
    "# Step 4: Display categorized products\n",
    "# Iterate through the dictionary and print items for each category.\n",
    "for category, items in category_dict.items():\n",
    "    print(f'{category}: {', '.join(items)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- defaultdict와 일반 딕셔너리의 차이\n",
    "    - 일반 딕셔너리를 사용할 때 발생하는 에러(KeyError)와 비교.\n",
    "    - defaultdict의 자동 초기화 기능이 실제 프로젝트에서 어떻게 유용한지 고민.\n",
    "\n",
    "- 데이터 분류의 구조화\n",
    "    - 데이터가 잘 분류되지 않았을 때 발생하는 문제점.\n",
    "    - 분류 기준이 모호하거나 데이터가 중첩되는 경우 어떻게 처리할지 고민.\n",
    "\n",
    "- 다른 데이터 구조와의 비교\n",
    "    - defaultdict 외에 데이터를 분류하기 위한 다른 방법(예: Pandas DataFrame)과 비교."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Employee Details:\n",
      "Name: Bob, Role: Manager, Salary: $95000\n",
      "Name: Charlie, Role: Designer, Salary: $70000\n",
      "Name: Alice, Role: Developer, Salary: $80000\n",
      "Total Salary: $245000\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "# Step 1: Define a namedtuple for employees\n",
    "# namedtuple crates a lightweight class for storing attributes.\n",
    "Employee = namedtuple(\"Employee\", [\"name\", \"role\", \"salary\"])\n",
    "\n",
    "# Step 2: List of employees\n",
    "# Each employee is represented as and instance of the Employee namedtuple.\n",
    "employees = {\n",
    "    Employee('Alice', 'Developer', 80000),\n",
    "    Employee('Bob', 'Manager', 95000),\n",
    "    Employee('Charlie', 'Designer', 70000)\n",
    "}\n",
    "\n",
    "# Step 3: Calculate the total salary\n",
    "# Use a generator expression to sum the salaries of all employees.\n",
    "total_salary = sum(emp.salary for emp in employees)\n",
    "\n",
    "# Step 4: Display employee details\n",
    "# Iterate through the list and display each employees's details.\n",
    "print(\"Employee Details:\")\n",
    "for emp in employees:\n",
    "    print(f'Name: {emp.name}, Role: {emp.role}, Salary: ${emp.salary}')\n",
    "print(f'Total Salary: ${total_salary}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- namedtuple의 장점\n",
    "    - 왜 일반 튜플이나 딕셔너리 대신 namedtuple을 사용하는지 고민\n",
    "    - 읽기 쉽고 직관적인 코드 작성의 중요성\n",
    "\n",
    "- 유사한 데이터 구조와의 비교\n",
    "    - namedtuple과 클래스를 사용하는 경우의 장단점 비교\n",
    "    - 프로젝트 규모나 데이터 복잡성에 따른 적합한 선택\n",
    "\n",
    "- 데이터 조작의 확장성\n",
    "    - namedtuple이 제공하는 불변성(immutable)이 코드 유지보수에 미치는 영향\n",
    "    - 불변 데이터 구조를 활용해야 할 상황과 그렇지 않은 상황의 구분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "    text: 0.00\n",
      "    mining: 0.00\n",
      "    algorithms: 0.41\n",
      "    analyze: 1.10\n",
      "Document 2:\n",
      "    text: 0.00\n",
      "    data: 1.10\n",
      "    mining: 0.00\n",
      "    finds: 1.10\n",
      "    patterns: 0.41\n",
      "    in: 0.41\n",
      "Document 3:\n",
      "    patterns: 0.41\n",
      "    and: 1.10\n",
      "    algorithms: 0.41\n",
      "    are: 1.10\n",
      "    used: 1.10\n",
      "    in: 0.41\n",
      "    text: 0.00\n",
      "    mining: 0.00\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "# Step 1: Sample documents\n",
    "# Each document is a string, representing some text content.\n",
    "documents = [\n",
    "    \"text mining algorithms analyze text\",\n",
    "    \"text data mining finds patterns in text\",\n",
    "    \"patterns and algorithms are used in text mining\"\n",
    "]\n",
    "\n",
    "# Step 2: Preprocess documents and calculate term frequencies (TF)\n",
    "# For each document, split it into words and count occurrences using Counter.\n",
    "tf = [Counter(doc.split()) for doc in documents]\n",
    "\n",
    "# Step 3: Calculate document frequency (DF) for each term\n",
    "# DF counts the number of documents containing each term.\n",
    "df = Counter()\n",
    "for doc_tf in tf:\n",
    "    df.update(doc_tf.keys())\n",
    "\n",
    "# Step 4: Calculate TF-IDF for each term in each document\n",
    "# Use the formula TF-IDF = TF * log(N / DF), where N is the total number of documents.\n",
    "N = len(documents)\n",
    "tf_idf = []\n",
    "for doc_index, doc_tf in enumerate(tf):\n",
    "    doc_tfidf = {}\n",
    "    for term, count in doc_tf.items():\n",
    "        idf = math.log(N / df[term])    # Inverse Document Frequency (IDF)\n",
    "        doc_tfidf[term] = count * idf\n",
    "    tf_idf.append(doc_tfidf)\n",
    "\n",
    "# Step 5: Display TF-IDF scores\n",
    "# Print TF-IDF scores for each document.\n",
    "for doc_index, scores in enumerate(tf_idf):\n",
    "    print(f'Document {doc_index + 1}:')\n",
    "    for term, score in scores.items():\n",
    "        print(f'    {term}: {score:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TF-IDF의 장단점\n",
    "    - 단어의 상대적 주요도를 평가할 수 있는 TF-IDF의 장점과 한계 고민.\n",
    "    - TF-IDF가 문맥이나 단어 간 관계를 반영하지 못하는 문제점 탐구.\n",
    "\n",
    "- IDF 계산 방식의 의의\n",
    "    - 왜 전체 문서 수와 단어가 등장한 문서 수의 비율을 로그로 계산하는지 이해.\n",
    "    - 로그 변환의 의미와 다른 스케일링 기법의 비교.\n",
    "\n",
    "- 다른 텍스트 분석 기법과의 비교\n",
    "    - Word2Vec, BERT 등의 임베딩 기법과 TF-IDF의 차이점.\n",
    "    - TF-IDF를 사용할 때와 임베딩 모델을 사용할 때의 적합한 상황 고민.\n",
    "\n",
    "- Counter 활용과 대규모 데이터\n",
    "    - Counter를 사용한 빈도 계산이 대규모 데이터에서 가지는 한계.\n",
    "    - 실제 대규모 데이터 처리 시 적합한 라이브러리(예: Scikit-learn, Pandas) 탐구."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
